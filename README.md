# Q-Learning Project

## Implementation Plan

Team Member: Katie Hughes

* Executing the Q-Learning Algorithm: I will follow the Q-Learning algorithm descirbed on the project page. There are 64 possible states (each of the 3 dumbbells can be at the origin or 1 of the three blocks, and you cannot have more than one dumbbell at the same block). There are 8 possible actions, which are moving a dumbbell from the origin to any of the blocks. At the moment I am thinking to represent the Q-Matrix as an array of size 64, with one entry per possible states. Each index of this array will contain a sub-array of size 8, representing the possible actions that can be taken next. When I initialize I will set every value to 0. Additionally I will need to check during the algorithm that the action is legal (for example, I can't move the blue dumbbell after it has already been moved, I can't move more than one dumbbell to the same location, etc). I will use the phantom robot movement script to fill out the Q-Matrix. 
* Determining when the Q-Matrix has converged: I plan to keep a counter within the main for-loop of the Q-Learning algorithm that increments by 1  if a change is not made, and is reset to 0 if a change is made. I wil have some threshold for number of iterations without a change at which I decide that the matrix has converged. 
* Once the Q-Matrix has converged, how to determine which actions the robot should take to maximize expected reward: I will continuously identify the current state of the robot and look it up in the Q-Matrix. Within this location I can see a different numbers that represent the rewards for different actions. I will select the action with the maximum reward and pass that onto the robot. This will continue until all dumbbells have been placed. 

* Determining identities and locations of the 3 colored dumbells: I will use the scan topic to locate the items closest to the robot. I will also subscribe to camera/rgb/image_raw to help identify the colors of the dumbbells. I will set a range for each color (red, blue, green) similarly to how we did in the class meeting 03 line follower code. With the combination of scan and camera data I can map a color to the dumbell location. 
* Determining identities and locations to the 3 numbered blocks: Similarly I will use the scan data to find the 3 closest items to the robot, once it is facing the other direction from the dumbbells. I'm currently unsure how to distinguish the blocks from each other as it doesn't seem that the camera can just read in the printed numbers. One idea I had was that there are different amounts of black on each cube, since each number has a specific shape, and maybe I could recognize this with the camera/rgb/image_raw data? This seems overly complicated, but I'm not sure if there is a simple way to convert an image to text in ROS. I tried looking this up and it seems like there is a package called cob_read_text which could potentially be used. 

* Picking up and putting down the dumbbells with the OpenManipulator arm
* Navigating to the appropriate locations to pick up and put down the dumbbells

## Timeline
I will try to complete these items in the order/grouping they are listed as. My first goal is to implement the Q-learning algorithm and fill in the Q-matrix. I hope to complete this by the end of this week (around Feb 20). Afterwards, I will try to implement the robot perception category items. I plan to complete this by mid-next week (around Feb 23/24). Finally I will work on the robot manipulation items, which I hope to complete by the end of next week (Feb 26/27) which will give me a few days to debug before the project is due. 
